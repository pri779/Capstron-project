# -*- coding: utf-8 -*-
"""Final capstron project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CerjdxZb2ZwsRxZ3FXqFsskq2IdEgU8f
"""

!pip install -q tensorflow pillow matplotlib streamlit pyngrok

from google.colab import drive
drive.mount('/content/drive')

!cp -r "/content/drive/MyDrive/waste" /content/waste

!ls /content/waste

!ls /content/waste/Hazardous

!ls /content/waste/Hazardous/Batteries | head

import pandas as pd

import pandas as pd

# ‚úÖ Create a clean, complete carbon emission mapping CSV
data = {
    "Category": [
        # Hazardous
        "Hazardous", "Hazardous", "Hazardous", "Hazardous", "Hazardous",
        # Non-Recyclable
        "Non-Recyclable", "Non-Recyclable", "Non-Recyclable", "Non-Recyclable", "Non-Recyclable", "Non-Recyclable",
        # Organic
        "Organic", "Organic", "Organic", "Organic", "Organic", "Organic",
        # Recyclable
        "Recyclable", "Recyclable", "Recyclable", "Recyclable", "Recyclable"
    ],
    "Subcategory": [
        # Hazardous
        "Batteries", "e-waste", "paints", "pesticides", "Hazardous",
        # Non-Recyclable
        "ceramic_product", "diapers", "plastic_bags_wrappers", "sanitary_napkin", "styrofoam_product", "Non-Recyclable",
        # Organic
        "coffee_tea_bags", "egg_shells", "food_scraps", "kitchen_waste", "yard_trimmings", "Organic",
        # Recyclable
        "cans_all_type", "glass_containers", "paper_products", "plastic_bottles", "Recyclable"
    ],
    "kg_CO2e_saved_per_item": [
        # Hazardous
        1.20, 0.90, 0.40, 0.30, 0.70,
        # Non-Recyclable
        0.02, 0.05, 0.03, 0.04, 0.01, 0.03,
        # Organic
        0.25, 0.10, 0.30, 0.35, 0.20, 0.24,
        # Recyclable
        0.15, 0.20, 0.07, 0.08, 0.13
    ],
    "notes": [
        # Hazardous
        "High chemical impact if not recycled properly",
        "Contains toxic metals like lead and mercury",
        "Should be disposed at hazardous waste centers",
        "Avoid contaminating soil and water",
        "Average CO‚ÇÇ saving across hazardous waste types",
        # Non-Recyclable
        "Difficult to recycle",
        "Not recyclable; dispose properly",
        "Single-use plastic waste",
        "Medical waste handling required",
        "Styrofoam is non-biodegradable",
        "Average CO‚ÇÇ saving across non-recyclable waste types",
        # Organic
        "Can be composted easily",
        "Compostable kitchen waste",
        "Compost to reduce methane",
        "Ideal for composting",
        "Biodegradable garden waste",
        "Average CO‚ÇÇ saving across compostable waste",
        # Recyclable
        "Aluminum cans save high energy when recycled",
        "Can be recycled into new glass products",
        "Paper recycling reduces deforestation",
        "Can be recycled into new plastic materials",
        "Average CO‚ÇÇ saving across recyclable items"
    ]
}

carbon_df = pd.DataFrame(data)
carbon_df.to_csv("/content/carbon_emission.csv", index=False)

print("‚úÖ 'carbon_emission.csv' file created successfully!")
display(carbon_df)

import os, shutil

# Path to your dataset
base_dir = "/content/waste"
flat_dir = "/content/waste_flat"
os.makedirs(flat_dir, exist_ok=True)

# Walk through all folders recursively
for root, dirs, files in os.walk(base_dir):
    for d in dirs:
        subfolder_path = os.path.join(root, d)
        # Check if this subfolder contains image files
        img_files = []
        for path, _, fns in os.walk(subfolder_path):
            for fn in fns:
                if fn.lower().endswith(('.jpg', '.jpeg', '.png')):
                    img_files.append(os.path.join(path, fn))
        if img_files:
            subcat_name = os.path.basename(subfolder_path.rstrip('/'))
            target_path = os.path.join(flat_dir, subcat_name)
            os.makedirs(target_path, exist_ok=True)
            for src in img_files:
                dst = os.path.join(target_path, os.path.basename(src))
                shutil.copy2(src, dst)

print("‚úÖ Dataset flattened successfully!")

# Show some sample files to confirm
!find /content/waste_flat -type f | head -20

from tensorflow.keras.preprocessing import image_dataset_from_directory

data_dir = "/content/waste_flat"
img_size = (128, 128)
batch_size = 32

train_ds = image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=img_size,
    batch_size=batch_size
)

val_ds = image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=img_size,
    batch_size=batch_size
)

print("‚úÖ Found", len(train_ds.class_names), "classes:", train_ds.class_names)

import os
import pandas as pd
import re
from difflib import get_close_matches
import inflect

# === Setup paths ===
flat_dir = "/content/waste_flat"
carbon_csv_path = "/content/carbon_emission.csv"
output_path = "/content/carbon_mapping.csv"

# === Load CSV ===
carbon_df = pd.read_csv(carbon_csv_path)

# === Normalization function ===
def normalize_name(name):
    return re.sub(r'[^a-z0-9]', '', str(name).lower().strip())

# Normalize subcategory names in CSV
carbon_df["norm_subcat"] = carbon_df["Subcategory"].apply(normalize_name)

# === Collect subcategories from folders ===
subcategories = []
for d in os.listdir(flat_dir):
    full_path = os.path.join(flat_dir, d)
    if os.path.isdir(full_path):
        has_images = any(
            fn.lower().endswith(('.jpg', '.jpeg', '.png'))
            for fn in os.listdir(full_path)
        )
        if has_images:
            subcategories.append(d)

print("‚úÖ Found actual image folders:", subcategories)

# === Initialize helper for singular/plural detection ===
p = inflect.engine()

mapping_rows = []
carbon_norm_list = carbon_df["norm_subcat"].tolist()

for sub in subcategories:
    norm_sub = normalize_name(sub)
    singular = p.singular_noun(norm_sub) or norm_sub

    # Try direct match
    match = carbon_df[carbon_df["norm_subcat"] == norm_sub]

    # Try singular match if no direct match
    if match.empty:
        match = carbon_df[carbon_df["norm_subcat"] == singular]

    # Try fuzzy match if still not found
    if match.empty:
        close_match = get_close_matches(norm_sub, carbon_norm_list, n=1, cutoff=0.8)
        if close_match:
            match = carbon_df[carbon_df["norm_subcat"] == close_match[0]]

    if not match.empty:
        row = match.iloc[0]
        category = row["Category"]
        carbon_saved = row["kg_CO2e_saved_per_item"]
        notes = row["notes"]
        mapping_rows.append([category, sub, carbon_saved, notes])
        print(f"‚úÖ Matched: {sub} ‚Üí {row['Subcategory']} ({category})")

    else:
        # Special handling for hazardous or unknowns
        if any(x in norm_sub for x in ["hazard", "paint", "battery", "pesticide", "ewaste"]):
            mapping_rows.append(["Hazardous", sub, 0.0, "Special handling required"])
            print(f"‚ö†Ô∏è  Marked as Hazardous: {sub}")
        else:
            mapping_rows.append(["Unknown", sub, 0.0, "No data available"])
            print(f"‚ùå No match found for: {sub}")

# === Save output ===
mapping_df = pd.DataFrame(mapping_rows, columns=["Category", "Subcategory", "kg_CO2e_saved_per_item", "notes"])
mapping_df.to_csv(output_path, index=False)

print("\n‚úÖ Cleaned carbon mapping file created successfully!")
print(f"üìÑ Saved to: {output_path}\n")

display(mapping_df)

import pandas as pd
import re
import os

# === Config paths ===
carbon_csv_path = "/content/carbon_emission.csv"

# === Helpers ===
def normalize_name(name):
    return re.sub(r'[^a-z0-9]', '', str(name).lower().strip())

# === Load & normalize CSV ===
carbon_df = pd.read_csv(carbon_csv_path, sep=None, engine='python')
carbon_df["norm_subcat"] = carbon_df["Subcategory"].apply(normalize_name)

# === Function to get CO2 info for any item ===
def get_waste_info(predicted_class: str, subcategory_name: str):
    pred_norm = normalize_name(predicted_class)
    sub_norm = normalize_name(subcategory_name)

    # Try to match normalized subcategory
    match = carbon_df[carbon_df["norm_subcat"] == sub_norm]

    # Try fuzzy match if not found (e.g., minor spelling issues)
    if match.empty:
        possible = carbon_df[carbon_df["norm_subcat"].str.contains(sub_norm[:5], na=False)]
        if not possible.empty:
            match = possible.iloc[[0]]

    if match.empty:
        # No match found in CSV
        return {
            "Predicted Class": predicted_class,
            "Subcategory": subcategory_name,
            "Mapped Category": "Unknown",
            "Recyclability": "Unknown ‚Äî not listed in carbon_emission.csv",
            "CO‚ÇÇ Saved (kg/item)": 0.0,
            "CO‚ÇÇ Note": "No CO‚ÇÇ data available.",
            "Notes": "No match found in CSV file."
        }

    # Extract info
    row = match.iloc[0]
    category = row["Category"]
    co2_saved = float(row["kg_CO2e_saved_per_item"])
    notes = row["notes"]

    # Determine recyclability phrase
    cat_lower = category.lower()
    if "hazard" in cat_lower:
        recyclability = "Hazardous ‚Äî requires special handling (may be recyclable at certified facilities)"
    elif "recyclable" in cat_lower:
        recyclability = "Recyclable"
    elif "non" in cat_lower and "recycl" in cat_lower:
        recyclability = "Non-Recyclable"
    elif "organic" in cat_lower:
        recyclability = "Organic ‚Äî compostable"
    else:
        recyclability = category

    # Build CO‚ÇÇ note
    co2_note = (
        f"Recycling or proper processing saves {co2_saved:.2f} kg CO‚ÇÇe per item. "
        f"If not recycled, approximately {co2_saved:.2f} kg CO‚ÇÇe more is emitted."
    )

    return {
        "Predicted Class": predicted_class,
        "Subcategory": subcategory_name,
        "Mapped Category": category,
        "Recyclability": recyclability,
        "CO‚ÇÇ Saved (kg/item)": co2_saved,
        "CO‚ÇÇ Note": co2_note,
        "Notes": notes
    }

# === Example usage for multiple predictions ===
if __name__ == "__main__":
    test_items = [
        ("Hazardous", "batteries"),
        ("Hazardous", "paints"),
        ("Recyclable", "plastic_bottles"),
        ("Organic", "food_scraps"),
        ("Non-Recyclable", "diapers"),
        ("Recyclable", "paper_products"),
        ("Organic", "yard_trimmings"),
    ]

    print("üîç Waste Classification Results:\n")
    for cls, sub in test_items:
        info = get_waste_info(cls, sub)
        print(f"Predicted Class: {info['Predicted Class']}")
        print(f"Subcategory: {info['Subcategory']}")
        print(f"Mapped Category: {info['Mapped Category']}")
        print(f"Recyclability: {info['Recyclability']}")
        print(f"CO‚ÇÇ Saved (kg/item): {info['CO‚ÇÇ Saved (kg/item)']}")
        print(f"CO‚ÇÇ Note: {info['CO‚ÇÇ Note']}")
        print(f"Notes: {info['Notes']}\n")

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ReduceLROnPlateau
import matplotlib.pyplot as plt
import pandas as pd

# Dataset path
train_dir = "/content/waste_flat"

# ‚úÖ Data Augmentation to improve generalization
datagen = ImageDataGenerator(
    rescale=1.0/255,
    validation_split=0.2,
    rotation_range=25,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.15,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

train_gen = datagen.flow_from_directory(
    train_dir,
    target_size=(128, 128),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

val_gen = datagen.flow_from_directory(
    train_dir,
    target_size=(128, 128),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

# ‚úÖ Transfer Learning with MobileNetV2
base_model = MobileNetV2(
    input_shape=(128, 128, 3),
    include_top=False,
    weights='imagenet'
)
base_model.trainable = False  # Freeze pretrained layers

# ‚úÖ Build final model
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.4),
    layers.Dense(train_gen.num_classes, activation='softmax')
])

# ‚úÖ Compile model (this was missing before)
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# ‚úÖ Learning rate scheduler to stabilize validation accuracy
lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    verbose=1,
    min_lr=1e-6
)

# ‚úÖ Train model
history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=10,
    callbacks=[lr_scheduler]
)

# ‚úÖ Save trained model
model.save("/content/waste_classifier_cnn_improved.h5")
print("‚úÖ Improved model trained and saved successfully!")

# ‚úÖ Plot training progress
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title('Training Progress (Improved Model)')
plt.show()

!pip install streamlit pyngrok==4.1.1 tensorflow pandas matplotlib

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import tensorflow as tf
# import numpy as np
# import pandas as pd
# from PIL import Image
# import os
# import re
# 
# # === Load model ===
# model_path = "/content/waste_classifier_cnn_improved.h5"
# model = tf.keras.models.load_model(model_path)
# 
# # === Load CO‚ÇÇ mapping CSV ===
# carbon_csv_path = "/content/carbon_mapping.csv"
# carbon_df = pd.read_csv(carbon_csv_path)
# 
# # === Normalize subcategory names for easier matching ===
# def normalize_name(name):
#     return re.sub(r'[^a-z0-9]', '', str(name).lower().strip())
# 
# carbon_df["norm_subcat"] = carbon_df["Subcategory"].apply(normalize_name)
# 
# # === Function to find info for predicted class ===
# def get_waste_info(predicted_class):
#     norm_pred = normalize_name(predicted_class)
#     match = carbon_df[carbon_df["norm_subcat"].str.contains(norm_pred, case=False, na=False)]
# 
#     # Handle partial match (e.g., singular/plural)
#     if match.empty:
#         match = carbon_df[carbon_df["norm_subcat"].apply(lambda x: norm_pred in x)]
# 
#     # üÜï If no subcategory match found, try matching the Category column directly
#     if match.empty:
#         cat_match = carbon_df[carbon_df["Category"].str.lower().str.contains(predicted_class.lower())]
#         if not cat_match.empty:
#             avg_co2 = cat_match["kg_CO2e_saved_per_item"].mean()
#             category = predicted_class
#             notes = f"Average CO‚ÇÇ savings across {len(cat_match)} '{category}' waste types."
# 
#             cat_lower = category.lower()
#             if "hazard" in cat_lower:
#                 recyclability = "Hazardous ‚Äî requires special handling"
#             elif "recyclable" in cat_lower:
#                 recyclability = "Recyclable"
#             elif "non" in cat_lower and "recycl" in cat_lower:
#                 recyclability = "Non-Recyclable"
#             elif "organic" in cat_lower:
#                 recyclability = "Organic ‚Äî compostable"
#             else:
#                 recyclability = category
# 
#             return {
#                 "Mapped Category": category,
#                 "Recyclability": recyclability,
#                 "CO‚ÇÇ Saved (kg/item)": avg_co2,
#                 "CO‚ÇÇ Note": f"Estimated average CO‚ÇÇ saving for {category} waste: {avg_co2:.2f} kg/item.",
#                 "Notes": notes
#             }
# 
#     # üß© Still nothing? Then unknown
#     if match.empty:
#         return {
#             "Mapped Category": "Unknown",
#             "Recyclability": "Unknown",
#             "CO‚ÇÇ Saved (kg/item)": 0.0,
#             "CO‚ÇÇ Note": "No CO‚ÇÇ data available.",
#             "Notes": "No match found in CSV file."
#         }
# 
#     # ‚úÖ Exact or close subcategory match found
#     row = match.iloc[0]
#     category = row["Category"]
#     co2_saved = float(row["kg_CO2e_saved_per_item"])
#     notes = row["notes"]
# 
#     cat_lower = category.lower()
#     if "hazard" in cat_lower:
#         recyclability = "Hazardous ‚Äî requires special handling (may be recyclable at certified facilities)"
#     elif "recyclable" in cat_lower:
#         recyclability = "Recyclable"
#     elif "non" in cat_lower and "recycl" in cat_lower:
#         recyclability = "Non-Recyclable"
#     elif "organic" in cat_lower:
#         recyclability = "Organic ‚Äî compostable"
#     else:
#         recyclability = category
# 
#     co2_note = (
#         f"Recycling or proper processing saves {co2_saved:.2f} kg CO‚ÇÇe per item. "
#         f"If not recycled, approximately {co2_saved:.2f} kg CO‚ÇÇe more is emitted."
#     )
# 
#     return {
#         "Mapped Category": category,
#         "Recyclability": recyclability,
#         "CO‚ÇÇ Saved (kg/item)": co2_saved,
#         "CO‚ÇÇ Note": co2_note,
#         "Notes": notes
#     }
# 
# 
# 
# # === Streamlit UI ===
# st.set_page_config(page_title="‚ôªÔ∏è Waste Classifier & CO‚ÇÇ Saver", layout="wide")
# st.title("‚ôªÔ∏è Smart Waste Classifier with Carbon Insights")
# st.write("Upload a waste image to detect its category and estimate CO‚ÇÇ savings.")
# 
# uploaded_file = st.file_uploader("üì§ Upload an Image", type=["jpg", "jpeg", "png"])
# 
# if uploaded_file:
#     image = Image.open(uploaded_file).convert("RGB")
#     st.image(image, caption="Uploaded Image",width=300)
# 
#     # === Prepare image for model ===
#     img = image.resize((128, 128))
#     img_array = np.expand_dims(np.array(img) / 255.0, axis=0)
# 
#     # === Predict class ===
#     pred = model.predict(img_array)
#     pred_class_idx = np.argmax(pred, axis=1)[0]
#     class_labels = sorted(os.listdir("/content/waste_flat"))
#     predicted_class = class_labels[pred_class_idx]
# 
#     st.subheader(f"üóë Predicted Class: **{predicted_class}**")
# 
#     # === Get CO‚ÇÇ info ===
#     info = get_waste_info(predicted_class)
# 
#     # === Display results ===
#     st.markdown("### ‚ôªÔ∏è Classification & CO‚ÇÇ Insights")
#     col1, col2 = st.columns(2)
#     with col1:
#         st.write(f"**Mapped Category:** {info['Mapped Category']}")
#         st.write(f"**Recyclability:** {info['Recyclability']}")
#     with col2:
#         st.metric("üåç CO‚ÇÇ Saved (kg/item)", value=info["CO‚ÇÇ Saved (kg/item)"])
#         st.write(info["CO‚ÇÇ Note"])
# 
#     st.info(f"üí° Notes: {info['Notes']}")
# 
# st.markdown("---")
# st.caption("Built with ‚ù§Ô∏è using TensorFlow, Streamlit, and sustainable data üå±")
#

!pip install -q streamlit pyngrok tensorflow pandas matplotlib

import subprocess, time, re
from pyngrok import ngrok

ngrok.set_auth_token("34gbBUuGibTMOdYv0DVE2izXqKq_6pPYuKRkxpoi6Tc5Z496Y")

log_path = "/content/streamlit.log"
process = subprocess.Popen(
    ["streamlit", "run", "app.py", "--server.port", "8501"],
    stdout=open(log_path, "w"),
    stderr=subprocess.STDOUT,
)

print("‚è≥ Starting Streamlit...", end="")
for _ in range(60):
    time.sleep(2)
    with open(log_path) as f:
        if re.search("You can now view your Streamlit app in your browser", f.read()):
            break
    print(".", end="")

public_url = ngrok.connect(8501)
print(f"\nüåç Streamlit app is live here: {public_url.public_url}")